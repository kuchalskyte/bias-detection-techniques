---
title: "Final Error Variance Analysis"
author: "Luka Kuchalskyte (s3983471)"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Error Variance Analysis

This bias detection method aims to identify disparities in error variance between different demographic groups, such as BLUE and RED individuals, to determine if the model's predictions are disproportionately impacting one group over the other.

1. **Stratified Bootstrapping**: Stratified bootstrapping ensures that each bootstrap sample maintains the same demographic composition, i.e., the proportion of RED and BLUE individuals is consistent across all samples. This helps maintain the diversity of the sample, making it more representative of the target population. N bootstrapped samples are generate, each with a demographic distribution matching the original simulated society, to simulate the variability that would occur in real-world scenarios where demographic makeup can shift.
2. **Model Training and Prediction**: A machine learning model is trained using the full sample (across all demographic groups). After training the model, predictions are made for each individual in the sample. The error is calculated as the difference between the actual outcome (ground truth) and the predicted value (Outcome - Prediction).
3. **Group-Based Error Variance Calculation**: The population is split into two groups based on their demographic characteristic (e.g., BLUE and RED individuals). This split allows us to examine whether the modelâ€™s errors are distributed equally across groups or if one group experiences larger errors (i.e., more variance). For each bootstrap sample, the variance of the errors is calculated separately for RED and BLUE individuals. This measures how spread out the errors are within each group. High variance in error rates may indicate that the model is less accurate or consistent for that group. This process is repeated for each bootstrap sample to account for random fluctuations and to ensure robustness in the error variance measurements.
4. **Statistical Analysis - ANOVA**: To formally compare the error variances between RED and BLUE individuals, an Analysis of Variance (ANOVA) test is applied. The ANOVA tests whether the differences in error variances between the two groups are statistically significant. A significant F-value would indicate that the model is treating the two demographic groups differently, with one group possibly facing higher prediction errors.
5. **Visualization - Heatmap of F-values**: A heatmap is generated to visualize the relationship between bias levels and the disparity in error variances across the two groups. The x-axis represents varying levels of bias introduced in the simulation (e.g., changes in income distribution between RED and BLUE individuals), while the y-axis shows the F-values from the ANOVA test. As bias increases in the simulated society (e.g., RED individuals are systematically disadvantaged in terms of income), the F-values from the ANOVA test are expected to increase. This increase indicates that larger error variances are emerging between the groups, signaling greater bias. A higher F-value suggests that the model's performance is more disparate across the two groups, with one group potentially suffering from higher prediction errors than the other.

```{r}
stratified_bootstrap <- function(data, variable1 = "Colour", 
                                 variable2 = "Disability", n_resamples = 100, 
                                 seed) {
  
  # In: data: A data frame containing the variables of interest;
  #         - Income: Simulated income values;
  #         - Education: Assigned education levels (0 = LOW, 1 = HIGH);
  #         - Colour: Assigned colour (0 = BLUE, 1 = RED);
  #         - Disability: Variable representing whether the individual 
  #                   has a disability (0 = no disability, 1 = has disability);
  #         - Outcome: Binary variable representing whether the individual 
  #                   experienced the undesired outcome (0 = desired, 1 = undesired);
  #     variable1: A string indicating the first categorical variable (default = "Colour");
  #     variable2: A string indicating the second categorical variable (default = "Disability");
  #     n_resamples: The number of bootstrap resamples to generate (default = 100);
  #     seed: The random seed used for the sampling.
  # Out:  A list of bootstrap samples maintaining the proportional representation of Colour and Disability.
  
  # Load required package
  library(dplyr)
  
  # Set a seed for reproducability
  set.seed(seed) 

  # Calculate the original proportions of each demographic group
  original_proportions <- data %>%
    group_by(!!sym(variable1), !!sym(variable2)) %>%
    summarise(n = n(), .groups = "drop") %>%
    mutate(proportion = n / sum(n))

  # Initialize an empty list to store the bootstrap samples
  bootstrap_samples <- list()

  # Perform stratified bootstrapping
  for (i in 1:n_resamples) {
    
    # Initialize an empty data frame for each bootstrap sample
    bootstrap_sample <- data.frame()

    # Resample within each demographic group to maintain group proportions
    for (j in 1:nrow(original_proportions)) {
      
      # Filter data for the current group
      group_data <- data %>%
        filter(!!sym(variable1) == original_proportions[[variable1]][j],
               !!sym(variable2) == original_proportions[[variable2]][j])

      # Calculate the number of samples to draw from this group
      n_samples <- round(original_proportions$proportion[j] * nrow(data))

      # Draw a bootstrap sample from the group with replacement
      group_resample <- group_data[sample(1:nrow(group_data), size = n_samples, 
                                          replace = TRUE), ]

      # Bind the resampled group data to the bootstrap sample
      bootstrap_sample <- rbind(bootstrap_sample, group_resample)
    }
    
    # Store the bootstrap sample in the list
    bootstrap_samples[[i]] <- bootstrap_sample
  }
  
  # Return the list of bootstrap samples
  return(bootstrap_samples)
}
```

```{r}
# Function to calculate prediction errors and perform ANOVA on variances
error_variance_analysis <- function(data, variable1 = "Colour", variable2 = "Disability", 
                                    n_resamples = 100, seed, formula = "Outcome ~ Income + Education", 
                                    family = binomial, response = "Outcome", type = "response") {
  
  # In: data: The dataset used for bootstrapping and modeling;
  #     variable1: A string indicating the sensitive variable (default = "Colour");
  #     variable2: A string indicating the sensitive variable (default = "Disability");
  #     n_resamples: Number of resamples for stratified bootstrapping (default = 100);
  #     seed: Seed for reproducibility;
  #     formula: A formula specifying the model (default = "Outcome ~ Income + Education");
  #     family: The family for the GLM model (default = binomial);
  #     response: The name of the response variable as a string (default = "Outcome");
  #     type: The type of prediction (default = "response").
  # Out: ANOVA results comparing error variances for RED and BLUE individuals.
  
  # Load required package
  library(ggplot2)

  # Generate bootstrap samples
  bootstrap_samples <- stratified_bootstrap(data = data, 
                                            variable1 = variable1, 
                                            variable2 = variable2, 
                                            n_resamples = n_resamples,
                                            seed = seed)

  # Initialize a data frame to store error variance results
  variance_results <- data.frame(Bootstrap = integer(), Group = character(), 
                                 Error_Variance = numeric())

  # Loop through each bootstrap sample
  for (i in 1:length(bootstrap_samples)) {
    
    # Get the current bootstrap sample
    sample <- bootstrap_samples[[i]]
    
    # Train the model 
    model <- glm(formula = formula,
                 data = sample,
                 family = family)
    
    # Get predictions
    pred <- predict(object = model,
                    newdata = sample,
                    type = type)  
    
    # Calculate errors (Outcome - Prediction)
    sample$Error <- sample$Outcome - pred
    
    # Calculate error variance for each group (e.g., Colour)
    for (group in unique(sample[[variable1]])) {
      group_data <- sample %>% filter(!!sym(variable1) == group)
      error_variance <- var(group_data$Error)  # Variance of errors for each group
      
      # Store the error variance results
      variance_results <- variance_results %>%
        add_row(Bootstrap = i, 
                Group = as.character(group), 
                Error_Variance = error_variance)
    }
  }

  variance_results$Group <- ifelse(variance_results$Group == 0, "BLUE", "RED")
  
  # Perform ANOVA to compare error variances between groups
  anova_result <- aov(Error_Variance ~ Group, data = variance_results)

  # Create a boxplot of prediction error variances by group
  p <- ggplot(variance_results, aes(x = Group, y = Error_Variance, fill = Group)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Exclude outliers for clarity
    geom_jitter(width = 0.2, color = "black", size = 1.5, alpha = 0.5) +  # Add jitter for individual data points
    labs(title = "Prediction Error Variances by Group (RED vs. BLUE)",
         x = "Group (RED vs. BLUE)",
         y = "Error Variance",
         caption = "Data represents bootstrap samples") +
    theme_minimal() +  # Use a minimal theme for a clean look
    scale_fill_manual(values = c("RED" = "red", "BLUE" = "blue")) +  # Custom colors for groups
    theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),  # Center and style title
          axis.title.x = element_text(size = 14, face = "bold"),
          axis.title.y = element_text(size = 14, face = "bold"),
          axis.text.x = element_text(size = 12),
          axis.text.y = element_text(size = 12),
          legend.position = "none")  # Remove legend for simplicity

  return(list(variance_results = variance_results,    # Error variance results 
              anova_result = summary(anova_result),   # ANOVA results
              visualisation = p))                     # Visualisation
}
```

```{r}
# Method C (Error Variance Analysis) Heatmap
errvar_analysis_heat <- function(societies, n_resamples = 100, seed, 
                                 formula = "Outcome ~ Income + Education") {
  
  # In: societies: List of societies to analyze;
  #     n_resamples: Number of resamples for stratified bootstrapping (default = 100);
  #     seed: Random seed for reproducibility;
  #     formula: Model formula (default = "Outcome ~ Income + Education").
  # Out: normalized_df: Data frame with the results of the error variance analysis;
  #      heatmap: Heatmap visualization for normalized error variances;
  #      visualisations: List containing visualisations.
  
  # Load library
  library(reshape2)
  
  # Placeholder list to store results and visualisations
  results_list <- list()
  visualisations_list <- list()
  
  # Run analysis for each society in the list
  for (society_name in names(societies)) {
    
    cat("Processing", society_name, "...\n")
    
    # Extract society data
    data <- societies[[society_name]]
    
    # Run error variance analysis
    results <- error_variance_analysis(data = data$data, 
                                       n_resamples = n_resamples,
                                       seed = seed, formula = formula)
    
    # Store results with society name
    results_list[[society_name]] <- data.frame(
      Society = society_name,
      Fvalue = results$anova_result[[1]][["F value"]][1])
    
    # Store visualisations for each society
    visualisations_list[[society_name]] <- list(
      Vis = results$visualisation)
  }
  
  # Combine results into a single data frame
  results_df <- do.call(rbind, results_list)
  rownames(results_df) <- NULL  # Reset row names
  results_df <- results_df[, c("Society", "Fvalue")]  # Ensure only necessary columns remain
  
  # Extract `b2` and `b3` from society names for visualisation
  results_df$b2 <- ifelse(
    results_df$Society == "baseline", 
    0.0, 
    as.numeric(sub("b2_([0-9.]+)_.*", "\\1", results_df$Society))
  )
  
  results_df$b3 <- ifelse(
    results_df$Society == "baseline", 
    0.0, 
    as.numeric(sub(".*_b3_([0-9.]+)", "\\1", results_df$Society))
  )
  
  # Normalize F-values (proportion of max F-value)
  max_fvalue <- max(results_df$Fvalue, na.rm = TRUE)
  results_df$Normalized_Fvalue <- results_df$Fvalue / max_fvalue
  
  # Melt the results_df for heatmap preparation
  heatmap_data <- melt(results_df, id.vars = c("Society", "b2", "b3"), 
                       measure.vars = c("Fvalue", "Normalized_Fvalue"),
                       variable.name = "Error_Type", value.name = "Value")
  
  # Plot heatmap for normalized F-values
  heatmap <- ggplot(subset(heatmap_data, Error_Type == "Normalized_Fvalue"), 
                    aes(x = b2, y = b3, fill = Value)) +
    geom_tile() +
    scale_fill_gradient(low = "blue", high = "red", name = "Normalized F-value") +
    labs(title = "Heatmap of Normalized F-values", 
         x = "Inc~Red Coef (b2)", y = "Out~Inc Coef (b3)") +
    theme_minimal()
  
  # Return the heatmap and results
  list(normalized_df = results_df,             # Normalized Wasserstein distances
       heatmap = heatmap,                      # Heatmap for normalized error variances
       visualisations = visualisations_list)   # Visualisations for each society
}
```

## 1.1 Scarce + Linear

```{r message=FALSE, warning=FALSE}
err_sl1 <- errvar_analysis_heat(societies = societies_s_l_1, seed = 3983471)
err_sl2 <- errvar_analysis_heat(societies = societies_s_l_2, seed = 3983471)
err_sl3 <- errvar_analysis_heat(societies = societies_s_l_3, seed = 3983471)
err_sl4 <- errvar_analysis_heat(societies = societies_s_l_4, seed = 3983471)
err_sl5 <- errvar_analysis_heat(societies = societies_s_l_5, seed = 3983471)
err_sl6 <- errvar_analysis_heat(societies = societies_s_l_6, seed = 3983471)
err_sl7 <- errvar_analysis_heat(societies = societies_s_l_7, seed = 3983471)
err_sl8 <- errvar_analysis_heat(societies = societies_s_l_8, seed = 3983471)
err_sl9 <- errvar_analysis_heat(societies = societies_s_l_9, seed = 3983471)
err_sl10 <- errvar_analysis_heat(societies = societies_s_l_10, seed = 3983471)
```

## 1.2 Scarce + Non-linear

```{r}
err_sn1 <- errvar_analysis_heat(societies = societies_s_n_1, seed = 3983471)
err_sn2 <- errvar_analysis_heat(societies = societies_s_n_2, seed = 3983471)
err_sn3 <- errvar_analysis_heat(societies = societies_s_n_3, seed = 3983471)
err_sn4 <- errvar_analysis_heat(societies = societies_s_n_4, seed = 3983471)
err_sn5 <- errvar_analysis_heat(societies = societies_s_n_5, seed = 3983471)
err_sn6 <- errvar_analysis_heat(societies = societies_s_n_6, seed = 3983471)
err_sn7 <- errvar_analysis_heat(societies = societies_s_n_7, seed = 3983471)
err_sn8 <- errvar_analysis_heat(societies = societies_s_n_8, seed = 3983471)
err_sn9 <- errvar_analysis_heat(societies = societies_s_n_9, seed = 3983471)
err_sn10 <- errvar_analysis_heat(societies = societies_s_n_10, seed = 3983471)
```

## 1.3 Non-scarce + Linear

```{r}
err_nl1 <- errvar_analysis_heat(societies = societies_n_l_1, seed = 3983471)
err_nl2 <- errvar_analysis_heat(societies = societies_n_l_2, seed = 3983471)
err_nl3 <- errvar_analysis_heat(societies = societies_n_l_3, seed = 3983471)
err_nl4 <- errvar_analysis_heat(societies = societies_n_l_4, seed = 3983471)
err_nl5 <- errvar_analysis_heat(societies = societies_n_l_5, seed = 3983471)
err_nl6 <- errvar_analysis_heat(societies = societies_n_l_6, seed = 3983471)
err_nl7 <- errvar_analysis_heat(societies = societies_n_l_7, seed = 3983471)
err_nl8 <- errvar_analysis_heat(societies = societies_n_l_8, seed = 3983471)
err_nl9 <- errvar_analysis_heat(societies = societies_n_l_9, seed = 3983471)
err_nl10 <- errvar_analysis_heat(societies = societies_n_l_10, seed = 3983471)
```

## 1.4 Non-scarce + Non-linear

```{r}
err_nn1 <- errvar_analysis_heat(societies = societies_n_n_1, seed = 3983471)
err_nn2 <- errvar_analysis_heat(societies = societies_n_n_2, seed = 3983471)
err_nn3 <- errvar_analysis_heat(societies = societies_n_n_3, seed = 3983471)
err_nn4 <- errvar_analysis_heat(societies = societies_n_n_4, seed = 3983471)
err_nn5 <- errvar_analysis_heat(societies = societies_n_n_5, seed = 3983471)
err_nn6 <- errvar_analysis_heat(societies = societies_n_n_6, seed = 3983471)
err_nn7 <- errvar_analysis_heat(societies = societies_n_n_7, seed = 3983471)
err_nn8 <- errvar_analysis_heat(societies = societies_n_n_8, seed = 3983471)
err_nn9 <- errvar_analysis_heat(societies = societies_n_n_9, seed = 3983471)
err_nn10 <- errvar_analysis_heat(societies = societies_n_n_10, seed = 3983471)
```

# 2. Correlations

```{r}
calculate_correlations <- function(data, b2_col, b3_col, measure_col, dataset_name) {
  
  # In: data: Normalized Wasserstein Distance data frame;
  #     b2_col: A string specifying the column name representing the first variable (e.g., "b2");
  #     b3_col: A string specifying the column name representing the second variable (e.g., "b3");
  #     measure_col: A string specifying the target or measurement column (e.g., "Pos_Mean_Nrm");
  #     dataset_name: A string representing the name of your dataset (e.g., "sl1").
  # Out: A tibble consisting of:
  #         - dataset: The provided dataset name.
  #         - cor_b2: Correlation between the average values of b2_col and measure_col.
  #         - cor_b3: Correlation between the average values of b3_col and measure_col.
  #         - cor_b2_b3: Correlation between the product of b2_col and b3_col and measure_col.
  
  # Load necessary library
  library(dplyr)
  
  # Step 1: Calculate average measure per group for b2_col
  avg_b3 <- data %>%
    group_by(!!sym(b2_col)) %>% # Group by b2 coefficient
    summarize(avg_measure_b3 = mean(!!sym(measure_col)), .groups = 'drop')

  # Calculate correlation between b2 and measure_col
  cor_b2 <- cor(avg_b3[[b2_col]], avg_b3$avg_measure_b3, use = "complete.obs")

  # Step 2: Calculate average measure per group for b3_col 
  avg_b2 <- data %>%
    group_by(!!sym(b3_col)) %>% # Group by b3 coefficient
    summarize(avg_measure_b2 = mean(!!sym(measure_col)), .groups = 'drop')

  # Calculate correlation between b3 and measure_col
  cor_b3 <- cor(avg_b2[[b3_col]], avg_b2$avg_measure_b2, use = "complete.obs")

  # Step 3: Compute product of b2 and b3, then calculate its correlation with measure_col
  data <- data %>%
    mutate(b2_b3 = !!sym(b2_col) * !!sym(b3_col))

  cor_b2_b3 <- cor(data$b2_b3, data[[measure_col]], use = "complete.obs")

  # Step 4: Return results as a tibble
  correlation_results <- data.frame(
    dataset = dataset_name,  # Dataset name for reference
    cor_b2 = cor_b2,         # Correlation between b2 and measure_col
    cor_b3 = cor_b3,         # Correlation between b3 and measure_col
    cor_b2_b3 = cor_b2_b3,   # Correlation between b2*b3 and measure_col
    stringsAsFactors = FALSE  # To ensure character columns don't become factors
  )
}
```

```{r}
library(purrr)

# List of datasets with names
datasets <- list(
  sl1 = err_sl1$normalized_df,
  sl2 = err_sl2$normalized_df,
  sl3 = err_sl3$normalized_df,
  sl4 = err_sl4$normalized_df,
  sl5 = err_sl5$normalized_df,
  sl6 = err_sl6$normalized_df,
  sl7 = err_sl7$normalized_df,
  sl8 = err_sl8$normalized_df,
  sl9 = err_sl9$normalized_df,
  sl10 = err_sl10$normalized_df,
  sn1 = err_sn1$normalized_df,
  sn2 = err_sn2$normalized_df,
  sn3 = err_sn3$normalized_df,
  sn4 = err_sn4$normalized_df,
  sn5 = err_sn5$normalized_df,
  sn6 = err_sn6$normalized_df,
  sn7 = err_sn7$normalized_df,
  sn8 = err_sn8$normalized_df,
  sn9 = err_sn9$normalized_df,
  sn10 = err_sn10$normalized_df,
  nl1 = err_nl1$normalized_df,
  nl2 = err_nl2$normalized_df,
  nl3 = err_nl3$normalized_df,
  nl4 = err_nl4$normalized_df,
  nl5 = err_nl5$normalized_df,
  nl6 = err_nl6$normalized_df,
  nl7 = err_nl7$normalized_df,
  nl8 = err_nl8$normalized_df,
  nl9 = err_nl9$normalized_df,
  nl10 = err_nl10$normalized_df,
  nn1 = err_nn1$normalized_df,
  nn2 = err_nn2$normalized_df,
  nn3 = err_nn3$normalized_df,
  nn4 = err_nn4$normalized_df,
  nn5 = err_nn5$normalized_df,
  nn6 = err_nn6$normalized_df,
  nn7 = err_nn7$normalized_df,
  nn8 = err_nn8$normalized_df,
  nn9 = err_nn9$normalized_df,
  nn10 = err_nn10$normalized_df
)

# Function to calculate correlations across datasets
calculate_results <- function(datasets, measure_col) {
  map2(datasets, names(datasets), ~ calculate_correlations(.x, "b2", "b3", measure_col, .y)) %>%
    bind_rows()
}

# Calculate err correlations
results_err <- calculate_results(datasets, "Normalized_Fvalue")
print(results_err)
```

```{r}
# Calculate average correlations
calculate_avg_correlations <- function(results_df) {
  
  # In: results_df:A dataframe containing the following columns:
  #         - dataset: A string representing the dataset name.
  #         - cor_b2: Correlation between b2 and measure_col.
  #         - cor_b3: Correlation between b3 and measure_col.
  #         - cor_b2_b3: Correlation between the product of b2 and b3 and measure_col.
  # Out: dataset: An abbreviated dataset name (the first two characters of the dataset).
  #      avg_cor_b2: The Fisher-Z inverse transformed average correlation between b2 and measure_col.
  #      avg_cor_b3: The Fisher-Z inverse transformed average correlation between b3 and measure_col.
  #      avg_cor_b2_b3: The Fisher-Z inverse transformed average correlation between the product of b2 and b3 and measure_col.
  
  # Load necessary library
  library (DescTools)
  
  # Transform dataset names to abbreviated form (first 2 characters)
  results_df %>%
    # Apply Fisher-Z transformation to correlations
    mutate(
      fisher_z_b2_to_avg_b3 = FisherZ(cor_b2),
      fisher_z_b3_to_avg_b2 = FisherZ(cor_b3),
      fisher_z_b2_b3_to_measure = FisherZ(cor_b2_b3)
    ) %>%
    # Group by abbreviated dataset name (first 2 characters)
    group_by(dataset = substr(dataset, 1, 2)) %>%
    summarize(
      avg_fisher_z_b2_to_avg_b3 = mean(fisher_z_b2_to_avg_b3, na.rm = TRUE),
      avg_fisher_z_b3_to_avg_b2 = mean(fisher_z_b3_to_avg_b2, na.rm = TRUE),
      avg_fisher_z_b2_b3_to_measure = mean(fisher_z_b2_b3_to_measure, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    # Apply Fisher-Z inverse transformation back to correlation scale
    mutate(
      avg_cor_b2 = FisherZInv(avg_fisher_z_b2_to_avg_b3),
      avg_cor_b3 = FisherZInv(avg_fisher_z_b3_to_avg_b2),
      avg_cor_b2_b3 = FisherZInv(avg_fisher_z_b2_b3_to_measure)
    ) %>%
    # Select relevant columns
    select(dataset, avg_cor_b2, avg_cor_b3, avg_cor_b2_b3)
}
```

```{r}
# Calculate average correlations for err
avg_results_err <- calculate_avg_correlations(results_err)

# Print results
print(avg_results_err)
```